"""
AI Model Server for Reusable Container Verification
FastAPI ì„œë²„ - ë‹¤íšŒìš©ê¸° ê²€ì¦ AI ì„œë¹„ìŠ¤
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from typing import List, Optional
import os
from dotenv import load_dotenv
from pathlib import Path

# ëª¨ë¸ import
from models.reusable_classifier import ReusableClassifierInference
from models.beverage_detector import BeverageDetectorInference
from models.embedding_generator import EmbeddingGenerator

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI(
    title="Reusable Container AI Service",
    description="AI ê¸°ë°˜ ë‹¤íšŒìš©ê¸° ê²€ì¦ ì„œë¹„ìŠ¤",
    version="0.2.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # TODO: í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
classifier: Optional[ReusableClassifierInference] = None
beverage_detector: Optional[BeverageDetectorInference] = None
embedding_generator: Optional[EmbeddingGenerator] = None


# Response Models
class ClassificationResponse(BaseModel):
    """ì¼íšŒìš©/ë‹¤íšŒìš© ë¶„ë¥˜ ì‘ë‹µ"""
    is_reusable: bool
    confidence: float
    predicted_class: str
    probabilities: dict
    message: str


class EmbeddingResponse(BaseModel):
    """ì„ë² ë”© ë²¡í„° ì‘ë‹µ"""
    embedding: List[float]
    dimension: int


class ContainerMatchResponse(BaseModel):
    """ìš©ê¸° ë§¤ì¹­ ì‘ë‹µ"""
    matches: List[dict]  # [{"cup_code": str, "similarity": float}, ...]
    top_match: Optional[dict]
    message: str


class BeverageVerificationResponse(BaseModel):
    """ìŒë£Œ ê²€ì¦ ì‘ë‹µ"""
    has_beverage: bool
    confidence: float
    predicted_class: str
    is_valid: bool
    probabilities: dict
    message: str


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”©"""
    global classifier, embedding_generator, beverage_detector

    print("ğŸš€ AI Model Server Starting...")

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = os.getenv('DEVICE', 'cpu')
    print(f"Device: {device}")

    # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    models_dir = Path("models/weights")
    classifier_path = models_dir / "reusable_classifier.pth"
    beverage_path = models_dir / "beverage_detector.pth"
    siamese_path = models_dir / "siamese_network.pth"
    embeddings_db_path = models_dir / "cup_code_embeddings_siamese.json"

    # Reusable Classifier ë¡œë“œ
    try:
        if classifier_path.exists():
            classifier = ReusableClassifierInference(
                model_path=str(classifier_path),
                device=device
            )
            print("âœ… Reusable classifier loaded")
        else:
            print(f"âš ï¸  Reusable classifier not found at {classifier_path}")
            print("   â†’ Train model using notebooks/01_reusable_classifier.ipynb")
    except Exception as e:
        print(f"âŒ Failed to load reusable classifier: {e}")

    # Beverage Detector ë¡œë“œ
    try:
        if beverage_path.exists():
            beverage_detector = BeverageDetectorInference(
                model_path=str(beverage_path),
                device=device,
                num_classes=3  # with_beverage, empty, unclear
            )
            print("âœ… Beverage detector loaded")
        else:
            print(f"âš ï¸  Beverage detector not found at {beverage_path}")
            print("   â†’ Train model using notebooks/03_beverage_detector.ipynb")
    except Exception as e:
        print(f"âŒ Failed to load beverage detector: {e}")

    # Siamese Network Embedding Generator ë¡œë“œ
    try:
        if siamese_path.exists():
            embedding_generator = EmbeddingGenerator(
                model_path=str(siamese_path),
                embeddings_db_path=str(embeddings_db_path) if embeddings_db_path.exists() else None,
                device=device,
                embedding_dim=256
            )
            print("âœ… Siamese Network embedding generator loaded")
        else:
            print(f"âš ï¸  Siamese Network not found at {siamese_path}")
            print("   â†’ Train model using notebooks/04_siamese_network_training.ipynb")
    except Exception as e:
        print(f"âŒ Failed to load embedding generator: {e}")

    print("\n" + "="*60)
    print("âœ… Server ready!")
    print("="*60)


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "AI Model Server is running",
        "status": "healthy",
        "version": "0.1.0"
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "device": os.getenv("DEVICE", "cpu"),
        "models_loaded": {
            "classifier": classifier is not None,
            "embedding_generator": embedding_generator is not None,
            "beverage_detector": beverage_detector is not None,
        }
    }


@app.post("/classify-reusable", response_model=ClassificationResponse)
async def classify_reusable(file: UploadFile = File(...)):
    """
    ë‹¤íšŒìš©ê¸° vs ì¼íšŒìš©ê¸° ë¶„ë¥˜

    ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ ë‹¤íšŒìš©ê¸°ì¸ì§€ ì¼íšŒìš©ê¸°ì¸ì§€ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    if classifier is None:
        raise HTTPException(
            status_code=503,
            detail="Classifier model not loaded. Please train the model first."
        )

    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await file.read()

        # ëª¨ë¸ ì¶”ë¡ 
        result = classifier.predict(image_bytes)

        # ë©”ì‹œì§€ ìƒì„±
        if result['is_reusable']:
            message = f"âœ… Reusable container detected (confidence: {result['confidence']:.1%})"
        else:
            message = f"âŒ Disposable container detected (confidence: {result['confidence']:.1%})"

        return ClassificationResponse(
            is_reusable=result['is_reusable'],
            confidence=result['confidence'],
            predicted_class=result['class'],
            probabilities=result['probabilities'],
            message=message
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")


@app.post("/generate-embedding", response_model=EmbeddingResponse)
async def generate_embedding(file: UploadFile = File(...)):
    """
    Siamese Networkë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì„ë² ë”© ë²¡í„° ìƒì„± (256ì°¨ì›)

    ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì—ì„œ L2-normalized 256ì°¨ì› ì„ë² ë”© ë²¡í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if embedding_generator is None:
        raise HTTPException(
            status_code=503,
            detail="Embedding generator not loaded. Please train the Siamese Network model first."
        )

    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await file.read()

        # ì„ë² ë”© ìƒì„±
        embedding = embedding_generator.generate_embedding(image_bytes)

        return EmbeddingResponse(
            embedding=embedding.tolist(),
            dimension=embedding_generator.get_embedding_dim()
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Embedding generation failed: {str(e)}")


@app.post("/match-container", response_model=ContainerMatchResponse)
async def match_container(
    file: UploadFile = File(...),
    threshold: float = 0.7,
    top_k: int = 3
):
    """
    ì´ë¯¸ì§€ì—ì„œ ìš©ê¸° ìœ í˜• ë§¤ì¹­

    ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë“±ë¡ëœ ìš©ê¸°ë“¤ê³¼ ë¹„êµí•˜ì—¬
    ê°€ì¥ ìœ ì‚¬í•œ ìš©ê¸°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

    Args:
        file: ì´ë¯¸ì§€ íŒŒì¼
        threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0, ê¸°ë³¸ê°’: 0.7)
        top_k: ë°˜í™˜í•  ìƒìœ„ ë§¤ì¹­ ê°œìˆ˜ (ê¸°ë³¸ê°’: 3)
    """
    if embedding_generator is None:
        raise HTTPException(
            status_code=503,
            detail="Embedding generator not loaded. Please train the Siamese Network model first."
        )

    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await file.read()

        # ìš©ê¸° ë§¤ì¹­
        matches = embedding_generator.match_container(
            image_bytes,
            threshold=threshold,
            top_k=top_k
        )

        # ì‘ë‹µ ìƒì„±
        if matches:
            matches_list = [
                {"cup_code": cup_code, "similarity": float(similarity)}
                for cup_code, similarity in matches
            ]
            top_match = matches_list[0]
            message = f"âœ… Matched to '{top_match['cup_code']}' with {top_match['similarity']:.1%} similarity"
        else:
            matches_list = []
            top_match = None
            message = f"âŒ No matching container found (threshold: {threshold:.1%})"

        return ContainerMatchResponse(
            matches=matches_list,
            top_match=top_match,
            message=message
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Container matching failed: {str(e)}")


@app.post("/verify-beverage", response_model=BeverageVerificationResponse)
async def verify_beverage(
    file: UploadFile = File(...),
    confidence_threshold: float = 0.7
):
    """
    ìŒë£Œ í¬í•¨ ì—¬ë¶€ ê²€ì¦

    ë‹¤íšŒìš©ê¸°ì— ìŒë£Œê°€ ë‹´ê²¨ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    ì‚¬ìš© ì¸ì¦ ì‹œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Args:
        file: ì´ë¯¸ì§€ íŒŒì¼
        confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.7)
    """
    if beverage_detector is None:
        raise HTTPException(
            status_code=503,
            detail="Beverage detector model not loaded. Please train the model first."
        )

    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await file.read()

        # ëª¨ë¸ ì¶”ë¡ 
        result = beverage_detector.predict(image_bytes, confidence_threshold)

        return BeverageVerificationResponse(
            has_beverage=result['has_beverage'],
            confidence=result['confidence'],
            predicted_class=result['class'],
            is_valid=result['is_valid'],
            probabilities=result['probabilities'],
            message=result['message']
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level="info"
    )
