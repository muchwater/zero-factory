"""
AI Model Server for Reusable Container Verification
FastAPI ì„œë²„ - ë‹¤íšŒìš©ê¸° ê²€ì¦ AI ì„œë¹„ìŠ¤
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from typing import List, Optional
import os
from dotenv import load_dotenv
from pathlib import Path

# ëª¨ë¸ import
from models.reusable_classifier import ReusableClassifierInference
from models.beverage_detector import BeverageDetectorInference

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI(
    title="Reusable Container AI Service",
    description="AI ê¸°ë°˜ ë‹¤íšŒìš©ê¸° ê²€ì¦ ì„œë¹„ìŠ¤",
    version="0.2.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # TODO: í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
classifier: Optional[ReusableClassifierInference] = None
beverage_detector: Optional[BeverageDetectorInference] = None
embedding_generator = None  # TODO: ì„ë² ë”© ëª¨ë¸ì€ ì¶”í›„ êµ¬í˜„


# Response Models
class ClassificationResponse(BaseModel):
    """ì¼íšŒìš©/ë‹¤íšŒìš© ë¶„ë¥˜ ì‘ë‹µ"""
    is_reusable: bool
    confidence: float
    predicted_class: str
    probabilities: dict
    message: str


class EmbeddingResponse(BaseModel):
    """ì„ë² ë”© ë²¡í„° ì‘ë‹µ"""
    embedding: List[float]
    dimension: int


class BeverageVerificationResponse(BaseModel):
    """ìŒë£Œ ê²€ì¦ ì‘ë‹µ"""
    has_beverage: bool
    confidence: float
    predicted_class: str
    is_valid: bool
    probabilities: dict
    message: str


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”©"""
    global classifier, embedding_generator, beverage_detector

    print("ğŸš€ AI Model Server Starting...")

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = os.getenv('DEVICE', 'cpu')
    print(f"Device: {device}")

    # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    models_dir = Path("models/weights")
    classifier_path = models_dir / "reusable_classifier.pth"
    beverage_path = models_dir / "beverage_detector.pth"

    # Reusable Classifier ë¡œë“œ
    try:
        if classifier_path.exists():
            classifier = ReusableClassifierInference(
                model_path=str(classifier_path),
                device=device
            )
            print("âœ… Reusable classifier loaded")
        else:
            print(f"âš ï¸  Reusable classifier not found at {classifier_path}")
            print("   â†’ Train model using notebooks/01_reusable_classifier.ipynb")
    except Exception as e:
        print(f"âŒ Failed to load reusable classifier: {e}")

    # Beverage Detector ë¡œë“œ
    try:
        if beverage_path.exists():
            beverage_detector = BeverageDetectorInference(
                model_path=str(beverage_path),
                device=device,
                num_classes=3  # with_beverage, empty, unclear
            )
            print("âœ… Beverage detector loaded")
        else:
            print(f"âš ï¸  Beverage detector not found at {beverage_path}")
            print("   â†’ Train model using notebooks/03_beverage_detector.ipynb")
    except Exception as e:
        print(f"âŒ Failed to load beverage detector: {e}")

    # TODO: Embedding Generator ë¡œë“œ
    print("âš ï¸  Embedding generator not implemented yet")

    print("\n" + "="*60)
    print("âœ… Server ready!")
    print("="*60)


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "AI Model Server is running",
        "status": "healthy",
        "version": "0.1.0"
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "device": os.getenv("DEVICE", "cpu"),
        "models_loaded": {
            "classifier": classifier is not None,
            "embedding_generator": embedding_generator is not None,
            "beverage_detector": beverage_detector is not None,
        }
    }


@app.post("/classify-reusable", response_model=ClassificationResponse)
async def classify_reusable(file: UploadFile = File(...)):
    """
    ë‹¤íšŒìš©ê¸° vs ì¼íšŒìš©ê¸° ë¶„ë¥˜

    ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ ë‹¤íšŒìš©ê¸°ì¸ì§€ ì¼íšŒìš©ê¸°ì¸ì§€ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    if classifier is None:
        raise HTTPException(
            status_code=503,
            detail="Classifier model not loaded. Please train the model first."
        )

    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await file.read()

        # ëª¨ë¸ ì¶”ë¡ 
        result = classifier.predict(image_bytes)

        # ë©”ì‹œì§€ ìƒì„±
        if result['is_reusable']:
            message = f"âœ… Reusable container detected (confidence: {result['confidence']:.1%})"
        else:
            message = f"âŒ Disposable container detected (confidence: {result['confidence']:.1%})"

        return ClassificationResponse(
            is_reusable=result['is_reusable'],
            confidence=result['confidence'],
            predicted_class=result['class'],
            probabilities=result['probabilities'],
            message=message
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")


@app.post("/generate-embedding", response_model=EmbeddingResponse)
async def generate_embedding(file: UploadFile = File(...)):
    """
    ì´ë¯¸ì§€ ì„ë² ë”© ë²¡í„° ìƒì„± (512ì°¨ì›)
    TODO: ì‹¤ì œ êµ¬í˜„
    """
    try:
        # TODO: ì‹¤ì œ ëª¨ë¸ ì¶”ë¡ 
        dummy_embedding = [0.0] * 512
        return EmbeddingResponse(
            embedding=dummy_embedding,
            dimension=512
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/verify-beverage", response_model=BeverageVerificationResponse)
async def verify_beverage(
    file: UploadFile = File(...),
    confidence_threshold: float = 0.7
):
    """
    ìŒë£Œ í¬í•¨ ì—¬ë¶€ ê²€ì¦

    ë‹¤íšŒìš©ê¸°ì— ìŒë£Œê°€ ë‹´ê²¨ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    ì‚¬ìš© ì¸ì¦ ì‹œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Args:
        file: ì´ë¯¸ì§€ íŒŒì¼
        confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.7)
    """
    if beverage_detector is None:
        raise HTTPException(
            status_code=503,
            detail="Beverage detector model not loaded. Please train the model first."
        )

    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await file.read()

        # ëª¨ë¸ ì¶”ë¡ 
        result = beverage_detector.predict(image_bytes, confidence_threshold)

        return BeverageVerificationResponse(
            has_beverage=result['has_beverage'],
            confidence=result['confidence'],
            predicted_class=result['class'],
            is_valid=result['is_valid'],
            probabilities=result['probabilities'],
            message=result['message']
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level="info"
    )
