{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 메모리 관리 유틸리티\n",
    "\n",
    "이 노트북의 셀들을 복사해서 다른 노트북에서 사용하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치: 이 셀을 다른 노트북에 복사하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory(verbose=True):\n",
    "    \"\"\"\n",
    "    GPU 메모리를 완전히 정리합니다.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        if verbose:\n",
    "            print(\"CUDA not available. No GPU memory to clear.\")\n",
    "        return None\n",
    "\n",
    "    # 정리 전 메모리 상태\n",
    "    before_allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "    before_reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"GPU Memory Cleanup\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Before cleanup:\")\n",
    "        print(f\"  Allocated: {before_allocated:.2f} MB\")\n",
    "        print(f\"  Reserved:  {before_reserved:.2f} MB\")\n",
    "\n",
    "    # 1. Python garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # 2. PyTorch 캐시 정리\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 3. 모든 CUDA 스트림 동기화\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 4. 가능하면 메모리 통계 리셋\n",
    "    try:\n",
    "        torch.cuda.reset_peak_memory_stats(0)\n",
    "        torch.cuda.reset_accumulated_memory_stats(0)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 정리 후 메모리 상태\n",
    "    after_allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "    after_reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "\n",
    "    freed_allocated = before_allocated - after_allocated\n",
    "    freed_reserved = before_reserved - after_reserved\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nAfter cleanup:\")\n",
    "        print(f\"  Allocated: {after_allocated:.2f} MB (freed: {freed_allocated:.2f} MB)\")\n",
    "        print(f\"  Reserved:  {after_reserved:.2f} MB (freed: {freed_reserved:.2f} MB)\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    return {\n",
    "        'before': {'allocated': before_allocated, 'reserved': before_reserved},\n",
    "        'after': {'allocated': after_allocated, 'reserved': after_reserved},\n",
    "        'freed': {'allocated': freed_allocated, 'reserved': freed_reserved}\n",
    "    }\n",
    "\n",
    "\n",
    "def print_gpu_memory(device_id=0):\n",
    "    \"\"\"\n",
    "    현재 GPU 메모리 상태를 출력합니다.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available\")\n",
    "        return\n",
    "\n",
    "    allocated = torch.cuda.memory_allocated(device_id) / 1024**2\n",
    "    reserved = torch.cuda.memory_reserved(device_id) / 1024**2\n",
    "    total = torch.cuda.get_device_properties(device_id).total_memory / 1024**2\n",
    "    free = total - allocated\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU {device_id}: {torch.cuda.get_device_name(device_id)}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Memory:     {total:.2f} MB\")\n",
    "    print(f\"Allocated:        {allocated:.2f} MB ({allocated/total*100:.1f}%)\")\n",
    "    print(f\"Reserved:         {reserved:.2f} MB ({reserved/total*100:.1f}%)\")\n",
    "    print(f\"Free:             {free:.2f} MB ({free/total*100:.1f}%)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 경고 메시지\n",
    "    if allocated / total > 0.9:\n",
    "        print(\"⚠️  WARNING: GPU memory usage is very high (>90%)\")\n",
    "    elif allocated / total > 0.8:\n",
    "        print(\"⚠️  CAUTION: GPU memory usage is high (>80%)\")\n",
    "\n",
    "\n",
    "print(\"✓ GPU memory management functions loaded\")\n",
    "print(\"  - clear_gpu_memory(): GPU 메모리 정리\")\n",
    "print(\"  - print_gpu_memory(): GPU 메모리 상태 확인\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 상태 확인\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 정리\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 삭제 및 메모리 정리 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 메모리를 차지하고 있다면 삭제\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "    print(\"✓ Model deleted\")\n",
    "\n",
    "# GPU 메모리 정리\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 긴급 메모리 정리 (모든 변수 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ 경고: 이 셀은 노트북의 모든 변수를 삭제합니다!\n",
    "# GPU 메모리가 심각하게 부족할 때만 사용하세요.\n",
    "\n",
    "import gc\n",
    "\n",
    "# 주요 객체 삭제\n",
    "vars_to_delete = ['model', 'optimizer', 'train_loader', 'val_loader', \n",
    "                  'train_dataset', 'val_dataset']\n",
    "\n",
    "for var_name in vars_to_delete:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "        print(f\"✓ Deleted {var_name}\")\n",
    "\n",
    "# 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n✓ Emergency cleanup completed\")\n",
    "print_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
