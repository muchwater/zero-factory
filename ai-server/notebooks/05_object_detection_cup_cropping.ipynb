{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Object Detection for Cup Cropping\n",
        "\n",
        "이 노트북에서는 Object Detection 모델을 사용하여 이미지에서 컵 영역을 감지하고 cropping합니다.\n",
        "\n",
        "## 목표\n",
        "- YOLOv8을 사용하여 컵 영역 감지\n",
        "- 감지된 컵 영역만 cropping하여 Siamese Network에 입력\n",
        "- 배경 noise 제거로 임베딩 품질 향상\n",
        "\n",
        "## 파이프라인\n",
        "```\n",
        "원본 이미지 → Object Detection → Crop → Siamese Network → 임베딩\n",
        "```\n",
        "\n",
        "## 사용 모델\n",
        "- **Object Detection**: YOLOv8n (nano, 빠르고 가벼움)\n",
        "- **Custom Training**: 컵 데이터셋으로 fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "# Ultralytics YOLO\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    print(\"✓ Ultralytics YOLO imported\")\n",
        "except ImportError:\n",
        "    print(\"Installing ultralytics...\")\n",
        "    !pip install ultralytics\n",
        "    from ultralytics import YOLO\n",
        "    print(\"✓ Ultralytics YOLO installed and imported\")\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 경로\n",
        "DATA_DIR = Path('./data/dataset_20251110_141636/types')  # cup_code별 디렉토리\n",
        "OUTPUT_DIR = Path('./data/cropped_cups')  # cropping된 이미지 저장 경로\n",
        "MODEL_SAVE_DIR = Path('./models/weights')\n",
        "\n",
        "# YOLO 설정\n",
        "YOLO_MODEL = 'yolov8n.pt'  # YOLOv8 nano (가볍고 빠름)\n",
        "# YOLO_MODEL = 'yolov8s.pt'  # YOLOv8 small (더 정확)\n",
        "CONFIDENCE_THRESHOLD = 0.25  # 감지 신뢰도 임계값\n",
        "IOU_THRESHOLD = 0.45  # NMS IoU 임계값\n",
        "\n",
        "# Cropping 설정\n",
        "CROP_PADDING = 0.1  # bbox 주변에 추가할 padding (비율)\n",
        "MIN_CROP_SIZE = 50  # 최소 crop 크기 (픽셀)\n",
        "\n",
        "# 클래스 필터 (cup, bottle 등)\n",
        "# COCO 데이터셋의 cup 클래스 ID: 41 (cup), 39 (bottle), 42 (wine glass)\n",
        "TARGET_CLASSES = [41, 39, 42, 44]  # cup, bottle, wine glass, bowl\n",
        "CLASS_NAMES = {41: 'cup', 39: 'bottle', 42: 'wine_glass', 44: 'bowl'}\n",
        "\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"YOLO model: {YOLO_MODEL}\")\n",
        "print(f\"Target classes: {[CLASS_NAMES.get(c, c) for c in TARGET_CLASSES]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. YOLO 모델 로드\n",
        "\n",
        "COCO 데이터셋으로 사전 학습된 YOLOv8 모델을 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO 모델 로드\n",
        "print(f\"Loading YOLO model: {YOLO_MODEL}...\")\n",
        "model = YOLO(YOLO_MODEL)\n",
        "\n",
        "# COCO 클래스 이름 확인\n",
        "print(f\"\\n✓ YOLO model loaded\")\n",
        "print(f\"Model has {len(model.names)} classes\")\n",
        "print(f\"\\nTarget classes for cup detection:\")\n",
        "for class_id in TARGET_CLASSES:\n",
        "    if class_id < len(model.names):\n",
        "        print(f\"  - {class_id}: {model.names[class_id]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 감지 및 Cropping 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_and_crop_cup(image_path, model, conf_thresh=0.25, iou_thresh=0.45, \n",
        "                        target_classes=None, padding=0.1, min_size=50, visualize=False):\n",
        "    \"\"\"\n",
        "    이미지에서 컵을 감지하고 cropping합니다.\n",
        "    \n",
        "    Args:\n",
        "        image_path: 입력 이미지 경로\n",
        "        model: YOLO 모델\n",
        "        conf_thresh: 감지 신뢰도 임계값\n",
        "        iou_thresh: NMS IoU 임계값\n",
        "        target_classes: 감지할 클래스 ID 리스트 (None이면 모든 클래스)\n",
        "        padding: bbox 주변 padding 비율\n",
        "        min_size: 최소 crop 크기 (픽셀)\n",
        "        visualize: 시각화 여부\n",
        "    \n",
        "    Returns:\n",
        "        cropped_images: cropping된 이미지 리스트 (PIL.Image)\n",
        "        detections: 감지 결과 리스트 (dict)\n",
        "    \"\"\"\n",
        "    # 이미지 로드\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    img_width, img_height = image.size\n",
        "    \n",
        "    # YOLO 감지\n",
        "    results = model.predict(\n",
        "        source=image_path,\n",
        "        conf=conf_thresh,\n",
        "        iou=iou_thresh,\n",
        "        classes=target_classes,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    cropped_images = []\n",
        "    detections = []\n",
        "    \n",
        "    # 결과 처리\n",
        "    if len(results) > 0 and len(results[0].boxes) > 0:\n",
        "        boxes = results[0].boxes\n",
        "        \n",
        "        for i, box in enumerate(boxes):\n",
        "            # Bounding box 좌표 (xyxy 형식)\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "            confidence = float(box.conf[0].cpu().numpy())\n",
        "            class_id = int(box.cls[0].cpu().numpy())\n",
        "            \n",
        "            # Padding 적용\n",
        "            width = x2 - x1\n",
        "            height = y2 - y1\n",
        "            \n",
        "            pad_x = width * padding\n",
        "            pad_y = height * padding\n",
        "            \n",
        "            x1_padded = max(0, int(x1 - pad_x))\n",
        "            y1_padded = max(0, int(y1 - pad_y))\n",
        "            x2_padded = min(img_width, int(x2 + pad_x))\n",
        "            y2_padded = min(img_height, int(y2 + pad_y))\n",
        "            \n",
        "            # 최소 크기 체크\n",
        "            crop_width = x2_padded - x1_padded\n",
        "            crop_height = y2_padded - y1_padded\n",
        "            \n",
        "            if crop_width < min_size or crop_height < min_size:\n",
        "                continue\n",
        "            \n",
        "            # Crop\n",
        "            cropped = image.crop((x1_padded, y1_padded, x2_padded, y2_padded))\n",
        "            cropped_images.append(cropped)\n",
        "            \n",
        "            # 감지 정보 저장\n",
        "            detections.append({\n",
        "                'bbox': (x1_padded, y1_padded, x2_padded, y2_padded),\n",
        "                'bbox_original': (int(x1), int(y1), int(x2), int(y2)),\n",
        "                'confidence': confidence,\n",
        "                'class_id': class_id,\n",
        "                'class_name': model.names[class_id]\n",
        "            })\n",
        "    \n",
        "    # 시각화\n",
        "    if visualize and len(detections) > 0:\n",
        "        visualize_detections(image, detections)\n",
        "    \n",
        "    return cropped_images, detections\n",
        "\n",
        "\n",
        "def visualize_detections(image, detections, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    감지 결과 시각화\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "    \n",
        "    # 이미지 복사 및 그리기\n",
        "    img_draw = image.copy()\n",
        "    draw = ImageDraw.Draw(img_draw)\n",
        "    \n",
        "    for det in detections:\n",
        "        bbox = det['bbox']\n",
        "        confidence = det['confidence']\n",
        "        class_name = det['class_name']\n",
        "        \n",
        "        # Bounding box 그리기\n",
        "        draw.rectangle(bbox, outline='red', width=3)\n",
        "        \n",
        "        # 라벨 그리기\n",
        "        label = f\"{class_name} {confidence:.2f}\"\n",
        "        draw.text((bbox[0], bbox[1] - 20), label, fill='red')\n",
        "    \n",
        "    ax.imshow(img_draw)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Detected {len(detections)} cups\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_crops(cropped_images, detections, max_display=5):\n",
        "    \"\"\"\n",
        "    Cropping된 이미지들 표시\n",
        "    \"\"\"\n",
        "    n_crops = min(len(cropped_images), max_display)\n",
        "    \n",
        "    if n_crops == 0:\n",
        "        print(\"No crops to display\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(1, n_crops, figsize=(4*n_crops, 4))\n",
        "    \n",
        "    if n_crops == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, (crop, det) in enumerate(zip(cropped_images[:n_crops], detections[:n_crops])):\n",
        "        axes[i].imshow(crop)\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f\"{det['class_name']}\\nConf: {det['confidence']:.2f}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"✓ Detection and cropping functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 샘플 이미지 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 샘플 이미지 선택\n",
        "sample_images = []\n",
        "for cup_code_dir in sorted(DATA_DIR.iterdir()):\n",
        "    if cup_code_dir.is_dir():\n",
        "        images = list(cup_code_dir.glob('*.jpg')) + list(cup_code_dir.glob('*.png'))\n",
        "        if len(images) > 0:\n",
        "            sample_images.append(images[0])\n",
        "\n",
        "print(f\"Found {len(sample_images)} sample images (1 per cup_code)\")\n",
        "\n",
        "# 첫 번째 샘플 테스트\n",
        "if len(sample_images) > 0:\n",
        "    test_image = sample_images[0]\n",
        "    print(f\"\\nTesting with: {test_image}\")\n",
        "    \n",
        "    cropped_images, detections = detect_and_crop_cup(\n",
        "        image_path=test_image,\n",
        "        model=model,\n",
        "        conf_thresh=CONFIDENCE_THRESHOLD,\n",
        "        iou_thresh=IOU_THRESHOLD,\n",
        "        target_classes=TARGET_CLASSES,\n",
        "        padding=CROP_PADDING,\n",
        "        min_size=MIN_CROP_SIZE,\n",
        "        visualize=True\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nDetected {len(detections)} cups\")\n",
        "    for i, det in enumerate(detections):\n",
        "        print(f\"  Cup {i+1}: {det['class_name']} (conf: {det['confidence']:.2f})\")\n",
        "    \n",
        "    if len(cropped_images) > 0:\n",
        "        show_crops(cropped_images, detections)\n",
        "else:\n",
        "    print(\"No sample images found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 여러 샘플 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여러 샘플 테스트\n",
        "n_samples = min(5, len(sample_images))\n",
        "\n",
        "print(f\"Testing {n_samples} samples...\\n\")\n",
        "\n",
        "for i, img_path in enumerate(sample_images[:n_samples]):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Sample {i+1}/{n_samples}: {img_path.parent.name}/{img_path.name}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    cropped_images, detections = detect_and_crop_cup(\n",
        "        image_path=img_path,\n",
        "        model=model,\n",
        "        conf_thresh=CONFIDENCE_THRESHOLD,\n",
        "        iou_thresh=IOU_THRESHOLD,\n",
        "        target_classes=TARGET_CLASSES,\n",
        "        padding=CROP_PADDING,\n",
        "        min_size=MIN_CROP_SIZE,\n",
        "        visualize=True\n",
        "    )\n",
        "    \n",
        "    if len(detections) == 0:\n",
        "        print(\"⚠️  No cups detected\")\n",
        "    else:\n",
        "        print(f\"✓ Detected {len(detections)} cups\")\n",
        "        show_crops(cropped_images, detections)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 전체 데이터셋 처리 및 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataset(data_dir, output_dir, model, conf_thresh, iou_thresh, \n",
        "                   target_classes, padding, min_size):\n",
        "    \"\"\"\n",
        "    전체 데이터셋 처리 및 저장\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    stats = {\n",
        "        'total_images': 0,\n",
        "        'images_with_detection': 0,\n",
        "        'total_crops': 0,\n",
        "        'no_detection': [],\n",
        "        'multiple_detections': []\n",
        "    }\n",
        "    \n",
        "    # Cup code별 디렉토리 순회\n",
        "    for cup_code_dir in sorted(data_dir.iterdir()):\n",
        "        if not cup_code_dir.is_dir():\n",
        "            continue\n",
        "        \n",
        "        cup_code = cup_code_dir.name\n",
        "        print(f\"\\nProcessing {cup_code}...\")\n",
        "        \n",
        "        # 출력 디렉토리 생성\n",
        "        cup_output_dir = output_dir / cup_code\n",
        "        cup_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # 이미지 파일 수집\n",
        "        image_files = list(cup_code_dir.glob('*.jpg')) + list(cup_code_dir.glob('*.png'))\n",
        "        \n",
        "        for img_path in tqdm(image_files, desc=f\"  {cup_code}\"):\n",
        "            stats['total_images'] += 1\n",
        "            \n",
        "            # 감지 및 cropping\n",
        "            cropped_images, detections = detect_and_crop_cup(\n",
        "                image_path=img_path,\n",
        "                model=model,\n",
        "                conf_thresh=conf_thresh,\n",
        "                iou_thresh=iou_thresh,\n",
        "                target_classes=target_classes,\n",
        "                padding=padding,\n",
        "                min_size=min_size,\n",
        "                visualize=False\n",
        "            )\n",
        "            \n",
        "            if len(cropped_images) == 0:\n",
        "                # 감지 실패 - 원본 이미지 저장\n",
        "                stats['no_detection'].append(str(img_path))\n",
        "                original = Image.open(img_path).convert('RGB')\n",
        "                output_path = cup_output_dir / img_path.name\n",
        "                original.save(output_path)\n",
        "            else:\n",
        "                stats['images_with_detection'] += 1\n",
        "                \n",
        "                if len(cropped_images) > 1:\n",
        "                    stats['multiple_detections'].append((str(img_path), len(cropped_images)))\n",
        "                \n",
        "                # Cropped 이미지 저장\n",
        "                for i, crop in enumerate(cropped_images):\n",
        "                    stats['total_crops'] += 1\n",
        "                    \n",
        "                    # 파일명: original_name_crop_0.jpg\n",
        "                    stem = img_path.stem\n",
        "                    ext = img_path.suffix\n",
        "                    output_name = f\"{stem}_crop_{i}{ext}\"\n",
        "                    output_path = cup_output_dir / output_name\n",
        "                    \n",
        "                    crop.save(output_path)\n",
        "    \n",
        "    return stats\n",
        "\n",
        "print(\"✓ Dataset processing function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 전체 데이터셋 처리\n",
        "print(f\"Processing entire dataset...\\n\")\n",
        "print(f\"Input:  {DATA_DIR}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print()\n",
        "\n",
        "stats = process_dataset(\n",
        "    data_dir=DATA_DIR,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    model=model,\n",
        "    conf_thresh=CONFIDENCE_THRESHOLD,\n",
        "    iou_thresh=IOU_THRESHOLD,\n",
        "    target_classes=TARGET_CLASSES,\n",
        "    padding=CROP_PADDING,\n",
        "    min_size=MIN_CROP_SIZE\n",
        ")\n",
        "\n",
        "# 통계 출력\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Processing Statistics\")\n",
        "print('='*60)\n",
        "print(f\"Total images processed: {stats['total_images']}\")\n",
        "print(f\"Images with detection: {stats['images_with_detection']} ({stats['images_with_detection']/stats['total_images']*100:.1f}%)\")\n",
        "print(f\"Total crops generated: {stats['total_crops']}\")\n",
        "print(f\"Images with no detection: {len(stats['no_detection'])}\")\n",
        "print(f\"Images with multiple detections: {len(stats['multiple_detections'])}\")\n",
        "\n",
        "if len(stats['no_detection']) > 0:\n",
        "    print(f\"\\n⚠️  Images with no detection:\")\n",
        "    for path in stats['no_detection'][:10]:  # Show first 10\n",
        "        print(f\"  - {path}\")\n",
        "    if len(stats['no_detection']) > 10:\n",
        "        print(f\"  ... and {len(stats['no_detection']) - 10} more\")\n",
        "\n",
        "if len(stats['multiple_detections']) > 0:\n",
        "    print(f\"\\nℹ️  Images with multiple detections:\")\n",
        "    for path, count in stats['multiple_detections'][:10]:  # Show first 10\n",
        "        print(f\"  - {path}: {count} cups\")\n",
        "    if len(stats['multiple_detections']) > 10:\n",
        "        print(f\"  ... and {len(stats['multiple_detections']) - 10} more\")\n",
        "\n",
        "print(f\"\\n✓ Cropped images saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cropped 이미지로 Siamese Network 학습\n",
        "\n",
        "이제 cropped 이미지로 Siamese Network를 재학습할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Siamese Network 재학습 안내\n",
        "print(\"다음 단계: Cropped 이미지로 Siamese Network 재학습\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"04_siamese_network_training.ipynb를 다시 실행하되,\")\n",
        "print(\"DATA_DIR을 다음으로 변경하세요:\")\n",
        "print()\n",
        "print(f\"  DATA_DIR = '{OUTPUT_DIR}'\")\n",
        "print()\n",
        "print(\"배경이 제거된 cropped 이미지로 학습하면:\")\n",
        "print(\"  ✓ Intra-class distance 감소 (같은 컵끼리 더 유사)\")\n",
        "print(\"  ✓ Inter-class distance 증가 (다른 컵끼리 더 차이)\")\n",
        "print(\"  ✓ 임베딩 품질 향상\")\n",
        "print(\"  ✓ 실전 성능 개선\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 통합 파이프라인 함수\n",
        "\n",
        "실제 서비스에서 사용할 수 있는 통합 파이프라인 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cup_detection_pipeline(image_path, yolo_model, siamese_model=None, \n",
        "                          conf_thresh=0.25, visualize=False):\n",
        "    \"\"\"\n",
        "    컵 감지 → Cropping → 임베딩 생성 파이프라인\n",
        "    \n",
        "    Args:\n",
        "        image_path: 입력 이미지 경로\n",
        "        yolo_model: YOLO 모델 (cup detection)\n",
        "        siamese_model: Siamese Network (embedding generation, optional)\n",
        "        conf_thresh: YOLO 신뢰도 임계값\n",
        "        visualize: 시각화 여부\n",
        "    \n",
        "    Returns:\n",
        "        results: {\n",
        "            'detections': list of detection dicts,\n",
        "            'cropped_images': list of PIL Images,\n",
        "            'embeddings': list of embeddings (if siamese_model provided)\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Step 1: Object Detection\n",
        "    cropped_images, detections = detect_and_crop_cup(\n",
        "        image_path=image_path,\n",
        "        model=yolo_model,\n",
        "        conf_thresh=conf_thresh,\n",
        "        iou_thresh=IOU_THRESHOLD,\n",
        "        target_classes=TARGET_CLASSES,\n",
        "        padding=CROP_PADDING,\n",
        "        min_size=MIN_CROP_SIZE,\n",
        "        visualize=visualize\n",
        "    )\n",
        "    \n",
        "    results = {\n",
        "        'detections': detections,\n",
        "        'cropped_images': cropped_images,\n",
        "        'embeddings': None\n",
        "    }\n",
        "    \n",
        "    # Step 2: Embedding Generation (optional)\n",
        "    if siamese_model is not None and len(cropped_images) > 0:\n",
        "        from torchvision import transforms\n",
        "        \n",
        "        # Transform for Siamese Network\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "        embeddings = []\n",
        "        for crop in cropped_images:\n",
        "            img_tensor = transform(crop).unsqueeze(0).to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                embedding = siamese_model(img_tensor)\n",
        "                embeddings.append(embedding.cpu().numpy()[0])\n",
        "        \n",
        "        results['embeddings'] = embeddings\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ Integrated pipeline function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 통합 파이프라인 테스트 (Siamese Network 포함)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Siamese Network 로드 (이미 학습된 모델이 있는 경우)\n",
        "try:\n",
        "    from models.siamese_network import SiameseNetwork\n",
        "    \n",
        "    siamese_model_path = MODEL_SAVE_DIR / 'siamese_network.pth'\n",
        "    \n",
        "    if siamese_model_path.exists():\n",
        "        print(f\"Loading Siamese Network from {siamese_model_path}...\")\n",
        "        \n",
        "        # 모델 초기화\n",
        "        siamese_model = SiameseNetwork(embedding_dim=256).to(device)\n",
        "        \n",
        "        # 체크포인트 로드\n",
        "        checkpoint = torch.load(siamese_model_path, map_location=device)\n",
        "        siamese_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        siamese_model.eval()\n",
        "        \n",
        "        print(\"✓ Siamese Network loaded\")\n",
        "        \n",
        "        # 테스트\n",
        "        if len(sample_images) > 0:\n",
        "            test_image = sample_images[0]\n",
        "            print(f\"\\nTesting pipeline with: {test_image}\")\n",
        "            \n",
        "            results = cup_detection_pipeline(\n",
        "                image_path=test_image,\n",
        "                yolo_model=model,\n",
        "                siamese_model=siamese_model,\n",
        "                conf_thresh=CONFIDENCE_THRESHOLD,\n",
        "                visualize=True\n",
        "            )\n",
        "            \n",
        "            print(f\"\\nPipeline Results:\")\n",
        "            print(f\"  Detections: {len(results['detections'])}\")\n",
        "            print(f\"  Cropped images: {len(results['cropped_images'])}\")\n",
        "            \n",
        "            if results['embeddings'] is not None:\n",
        "                print(f\"  Embeddings: {len(results['embeddings'])}\")\n",
        "                for i, emb in enumerate(results['embeddings']):\n",
        "                    print(f\"    - Embedding {i+1} shape: {emb.shape}\")\n",
        "                    print(f\"      L2 norm: {np.linalg.norm(emb):.4f}\")\n",
        "            \n",
        "            show_crops(results['cropped_images'], results['detections'])\n",
        "    else:\n",
        "        print(f\"Siamese model not found at {siamese_model_path}\")\n",
        "        print(\"Run 04_siamese_network_training.ipynb first to train the model\")\n",
        "        siamese_model = None\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not load Siamese Network: {e}\")\n",
        "    siamese_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 요약 및 다음 단계\n",
        "\n",
        "### 구현 내용\n",
        "1. ✅ YOLOv8을 사용한 컵 영역 감지\n",
        "2. ✅ 감지된 영역 cropping 및 저장\n",
        "3. ✅ 전체 데이터셋 일괄 처리\n",
        "4. ✅ Siamese Network와 통합 파이프라인\n",
        "\n",
        "### 장점\n",
        "- 배경 noise 제거로 임베딩 품질 향상\n",
        "- 컵 영역에만 집중하여 특징 추출\n",
        "- 실전 환경에서 더 robust한 성능\n",
        "\n",
        "### 다음 단계\n",
        "1. **Cropped 이미지로 Siamese Network 재학습**\n",
        "   - `04_siamese_network_training.ipynb`에서 `DATA_DIR` 변경\n",
        "   - 성능 비교 (원본 vs cropped)\n",
        "\n",
        "2. **FastAPI 서버 통합**\n",
        "   ```python\n",
        "   # /api/detect-and-embed 엔드포인트\n",
        "   1. YOLO로 컵 감지\n",
        "   2. Cropping\n",
        "   3. Siamese Network로 임베딩 생성\n",
        "   4. 데이터베이스와 비교\n",
        "   ```\n",
        "\n",
        "3. **Custom YOLO 학습 (선택사항)**\n",
        "   - 컵 데이터셋으로 fine-tuning\n",
        "   - 더 높은 정확도 달성\n",
        "\n",
        "4. **성능 최적화**\n",
        "   - YOLO 모델 경량화 (YOLOv8n)\n",
        "   - 추론 속도 개선\n",
        "   - GPU 메모리 최적화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 부록: YOLO Custom Training 가이드\n",
        "\n",
        "만약 COCO 모델의 성능이 충분하지 않다면, 컵 데이터셋으로 custom training을 수행할 수 있습니다.\n",
        "\n",
        "```python\n",
        "# 1. 데이터셋 준비 (YOLO 형식)\n",
        "# dataset/\n",
        "#   images/\n",
        "#     train/\n",
        "#     val/\n",
        "#   labels/\n",
        "#     train/\n",
        "#     val/\n",
        "#   data.yaml\n",
        "\n",
        "# 2. data.yaml 작성\n",
        "# path: ./dataset\n",
        "# train: images/train\n",
        "# val: images/val\n",
        "# nc: 1  # number of classes\n",
        "# names: ['cup']\n",
        "\n",
        "# 3. 학습\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')\n",
        "results = model.train(\n",
        "    data='dataset/data.yaml',\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    device=0\n",
        ")\n",
        "```\n",
        "\n",
        "Label Studio를 사용하여 annotation을 수행할 수 있습니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
