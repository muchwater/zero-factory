{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP 기반 임베딩 생성 모델\n",
    "\n",
    "## 목표\n",
    "이미지를 512차원 벡터로 변환하여 유사도 비교에 사용\n",
    "\n",
    "## 모델\n",
    "- **백본**: CLIP (openai/clip-vit-base-patch32)\n",
    "- **출력**: 512차원 L2 정규화 벡터\n",
    "- **용도**: 다회용기 이미지 간 유사도 계산\n",
    "\n",
    "## 특징\n",
    "- 사전학습된 모델 사용 (학습 불필요)\n",
    "- 코사인 유사도로 이미지 비교\n",
    "- 빠른 추론 속도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CLIP 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP 모델 및 프로세서 로드\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "print(f\"Loading CLIP model: {model_name}\")\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"Embedding dimension: 512\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 임베딩 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(image_path, model, processor, device):\n",
    "    \"\"\"\n",
    "    이미지에서 512차원 임베딩 벡터 생성\n",
    "    \n",
    "    Args:\n",
    "        image_path: 이미지 파일 경로\n",
    "        model: CLIP 모델\n",
    "        processor: CLIP 프로세서\n",
    "        device: 디바이스 (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: L2 정규화된 512차원 벡터\n",
    "    \"\"\"\n",
    "    # 이미지 로딩\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # 전처리\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    # L2 정규화 (코사인 유사도 계산 최적화)\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    \n",
    "    # numpy 배열로 변환\n",
    "    embedding = image_features.cpu().numpy().flatten()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def generate_embedding_batch(image_paths, model, processor, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    여러 이미지의 임베딩을 배치로 생성\n",
    "    \n",
    "    Args:\n",
    "        image_paths: 이미지 파일 경로 리스트\n",
    "        model: CLIP 모델\n",
    "        processor: CLIP 프로세서\n",
    "        device: 디바이스\n",
    "        batch_size: 배치 크기\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: (N, 512) 형태의 임베딩 행렬\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc='Generating embeddings'):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        \n",
    "        # 이미지 로딩\n",
    "        images = [Image.open(path).convert('RGB') for path in batch_paths]\n",
    "        \n",
    "        # 전처리\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        \n",
    "        # L2 정규화\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        embeddings.append(image_features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 유사도 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    두 임베딩 벡터 간 코사인 유사도 계산\n",
    "    \n",
    "    Args:\n",
    "        embedding1: 첫 번째 임베딩 벡터\n",
    "        embedding2: 두 번째 임베딩 벡터\n",
    "    \n",
    "    Returns:\n",
    "        float: 코사인 유사도 (0.0 ~ 1.0)\n",
    "    \"\"\"\n",
    "    # L2 정규화되어 있으면 내적이 곧 코사인 유사도\n",
    "    similarity = np.dot(embedding1, embedding2)\n",
    "    return float(similarity)\n",
    "\n",
    "def find_most_similar(query_embedding, database_embeddings, threshold=0.7):\n",
    "    \"\"\"\n",
    "    데이터베이스에서 가장 유사한 임베딩 찾기\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: 쿼리 임베딩 (512,)\n",
    "        database_embeddings: DB 임베딩 행렬 (N, 512)\n",
    "        threshold: 최소 유사도 임계값\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (가장 유사한 인덱스, 유사도) 또는 (None, 0.0)\n",
    "    \"\"\"\n",
    "    if len(database_embeddings) == 0:\n",
    "        return None, 0.0\n",
    "    \n",
    "    # 배치 코사인 유사도 계산\n",
    "    similarities = np.dot(database_embeddings, query_embedding)\n",
    "    \n",
    "    # 최대값 찾기\n",
    "    max_idx = np.argmax(similarities)\n",
    "    max_similarity = similarities[max_idx]\n",
    "    \n",
    "    # 임계값 체크\n",
    "    if max_similarity >= threshold:\n",
    "        return int(max_idx), float(max_similarity)\n",
    "    else:\n",
    "        return None, float(max_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 테스트: 단일 이미지 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지 경로 (직접 지정)\n",
    "test_image_path = '../data/test_images/tumbler1.jpg'\n",
    "\n",
    "# 임베딩 생성\n",
    "embedding = generate_embedding(test_image_path, model, processor, device)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding norm (should be ~1.0): {np.linalg.norm(embedding):.4f}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "\n",
    "# 이미지 표시\n",
    "img = Image.open(test_image_path)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Test Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 유사도 테스트: 같은 물체 vs 다른 물체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지 3개 준비\n",
    "# - tumbler1.jpg: 기준 텀블러\n",
    "# - tumbler2.jpg: 같은 텀블러 (다른 각도)\n",
    "# - cup.jpg: 다른 컵\n",
    "\n",
    "image1_path = '../data/test_images/tumbler1.jpg'\n",
    "image2_path = '../data/test_images/tumbler2.jpg'  # 같은 물체\n",
    "image3_path = '../data/test_images/cup.jpg'       # 다른 물체\n",
    "\n",
    "# 임베딩 생성\n",
    "emb1 = generate_embedding(image1_path, model, processor, device)\n",
    "emb2 = generate_embedding(image2_path, model, processor, device)\n",
    "emb3 = generate_embedding(image3_path, model, processor, device)\n",
    "\n",
    "# 유사도 계산\n",
    "sim_same = calculate_cosine_similarity(emb1, emb2)\n",
    "sim_diff = calculate_cosine_similarity(emb1, emb3)\n",
    "\n",
    "print(f\"Similarity (same object, different angle): {sim_same:.4f}\")\n",
    "print(f\"Similarity (different object): {sim_diff:.4f}\")\n",
    "print(f\"\\nDifference: {sim_same - sim_diff:.4f}\")\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(Image.open(image1_path))\n",
    "axes[0].set_title('Reference Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(Image.open(image2_path))\n",
    "axes[1].set_title(f'Same Object\\nSimilarity: {sim_same:.3f}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(Image.open(image3_path))\n",
    "axes[2].set_title(f'Different Object\\nSimilarity: {sim_diff:.3f}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. 데이터셋에서 cup_code별 임베딩 생성\n\n이제 `convert_labelstudio_to_dataset.py`로 생성된 데이터셋을 사용합니다.\n데이터셋 구조:\n```\ndataset.zip\n└── types/\n    ├── CUP001/\n    ├── CUP002/\n    └── ...\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import zipfile\nfrom collections import defaultdict\n\n# 데이터셋 경로 설정\ndataset_zip_path = '../dataset_output/dataset_20251110_120000.zip'  # 실제 파일명으로 변경\ntypes_dir = '../data/types'  # 압축 해제 위치\n\n# ZIP 파일에서 types/ 디렉토리만 추출\nif os.path.exists(dataset_zip_path):\n    print(f\"Extracting types/ from {dataset_zip_path}...\")\n    \n    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n        # types/ 디렉토리 내의 파일만 추출\n        types_files = [f for f in zip_ref.namelist() if f.startswith('types/')]\n        \n        for file in types_files:\n            zip_ref.extract(file, '../data/')\n    \n    print(f\"✓ Extracted {len(types_files)} files\")\nelse:\n    print(f\"⚠ Dataset not found: {dataset_zip_path}\")\n    print(\"Please run convert_labelstudio_to_dataset.py first with --include-types option\")\n\n# cup_code별로 이미지 경로 수집\ncup_code_images = defaultdict(list)\n\nif os.path.exists(types_dir):\n    for cup_code in os.listdir(types_dir):\n        cup_dir = os.path.join(types_dir, cup_code)\n        \n        if os.path.isdir(cup_dir):\n            images = [os.path.join(cup_dir, f) for f in os.listdir(cup_dir) \n                     if f.endswith(('.jpg', '.png', '.jpeg'))]\n            cup_code_images[cup_code] = images\n    \n    print(f\"\\nFound {len(cup_code_images)} cup codes:\")\n    for cup_code, images in sorted(cup_code_images.items()):\n        print(f\"  {cup_code}: {len(images)} images\")\nelse:\n    print(f\"⚠ Types directory not found: {types_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. cup_code별 임베딩 생성 및 저장"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# cup_code별로 임베딩 생성\ncup_code_embeddings = {}\n\nfor cup_code, image_paths in tqdm(cup_code_images.items(), desc='Processing cup codes'):\n    if len(image_paths) > 0:\n        # 배치 임베딩 생성\n        embeddings = generate_embedding_batch(image_paths, model, processor, device, batch_size=8)\n        \n        # 평균 임베딩 계산 (대표 임베딩)\n        mean_embedding = embeddings.mean(axis=0)\n        # 정규화\n        mean_embedding = mean_embedding / np.linalg.norm(mean_embedding)\n        \n        cup_code_embeddings[cup_code] = {\n            'mean_embedding': mean_embedding,\n            'all_embeddings': embeddings,\n            'image_paths': image_paths,\n            'num_images': len(image_paths)\n        }\n\nprint(f\"\\n✓ Generated embeddings for {len(cup_code_embeddings)} cup codes\")\nprint(f\"\\nSummary:\")\nfor cup_code, data in sorted(cup_code_embeddings.items()):\n    print(f\"  {cup_code}: {data['num_images']} images → mean embedding (512,)\")\n    print(f\"    Norm: {np.linalg.norm(data['mean_embedding']):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. cup_code 간 유사도 행렬 시각화"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import seaborn as sns\n\nif len(cup_code_embeddings) > 1:\n    # cup_code 리스트와 평균 임베딩 행렬 생성\n    cup_codes = sorted(cup_code_embeddings.keys())\n    mean_embeddings = np.array([cup_code_embeddings[cc]['mean_embedding'] for cc in cup_codes])\n    \n    # 유사도 행렬 계산\n    similarity_matrix = np.dot(mean_embeddings, mean_embeddings.T)\n    \n    # 시각화\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(similarity_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n                xticklabels=cup_codes,\n                yticklabels=cup_codes,\n                vmin=0, vmax=1)\n    plt.title('Cup Code Similarity Matrix (Mean Embeddings)')\n    plt.tight_layout()\n    plt.show()\n    \n    # 가장 유사한 cup_code 쌍 찾기\n    print(\"\\nMost similar cup code pairs:\")\n    for i in range(len(cup_codes)):\n        for j in range(i+1, len(cup_codes)):\n            sim = similarity_matrix[i, j]\n            if sim > 0.7:  # 임계값 이상만 출력\n                print(f\"  {cup_codes[i]} ↔ {cup_codes[j]}: {sim:.4f}\")\nelse:\n    print(\"Need at least 2 cup codes for similarity matrix\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. 실전 시나리오: 촬영 이미지로 cup_code 매칭"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 시나리오:\n# 1. DB에 cup_code별 대표 임베딩 저장\n# 2. 사용자가 촬영한 이미지로 어떤 cup_code인지 매칭\n\n# 쿼리 이미지 선택 (데이터셋에서 임의로 하나 선택)\nif len(cup_code_embeddings) > 0:\n    # 첫 번째 cup_code의 첫 번째 이미지를 쿼리로 사용\n    test_cup_code = list(cup_code_embeddings.keys())[0]\n    test_image_path = cup_code_embeddings[test_cup_code]['image_paths'][0]\n    \n    print(f\"Test query image: {test_image_path}\")\n    print(f\"Expected cup_code: {test_cup_code}\")\n    \n    # 쿼리 임베딩 생성\n    query_embedding = generate_embedding(test_image_path, model, processor, device)\n    \n    # DB 임베딩 준비 (cup_code별 평균 임베딩)\n    db_cup_codes = list(cup_code_embeddings.keys())\n    db_embeddings = np.array([cup_code_embeddings[cc]['mean_embedding'] for cc in db_cup_codes])\n    \n    # 매칭\n    matched_idx, similarity = find_most_similar(query_embedding, db_embeddings, threshold=0.7)\n    \n    print(\"\\n=== Matching Result ===\")\n    if matched_idx is not None:\n        matched_cup_code = db_cup_codes[matched_idx]\n        print(f\"✓ Matched cup_code: {matched_cup_code}\")\n        print(f\"  Similarity: {similarity:.4f}\")\n        print(f\"  Correct: {'Yes' if matched_cup_code == test_cup_code else 'No'}\")\n    else:\n        print(f\"✗ No match found\")\n        print(f\"  Highest similarity: {similarity:.4f}\")\n    \n    # 상위 3개 결과 출력\n    print(\"\\nTop 3 matches:\")\n    similarities = np.dot(db_embeddings, query_embedding)\n    top3_indices = np.argsort(similarities)[::-1][:3]\n    \n    for rank, idx in enumerate(top3_indices, 1):\n        print(f\"  {rank}. {db_cup_codes[idx]}: {similarities[idx]:.4f}\")\n    \n    # 이미지 표시\n    img = Image.open(test_image_path)\n    plt.figure(figsize=(6, 6))\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(f'Query Image\\nExpected: {test_cup_code}\\nMatched: {db_cup_codes[matched_idx] if matched_idx is not None else \"None\"}')\n    plt.show()\nelse:\n    print(\"No cup_code embeddings available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 임계값 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 임계값에서 매칭 결과 분석\n",
    "thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "\n",
    "if len(registered_embeddings) > 0 and os.path.exists(query_path):\n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        matched_idx, similarity = find_most_similar(query_embedding, registered_embeddings, threshold=threshold)\n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'matched': matched_idx is not None,\n",
    "            'similarity': similarity\n",
    "        })\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, [r['similarity'] for r in results], 'b-', linewidth=2, label='Similarity Score')\n",
    "    plt.axhline(y=0.7, color='r', linestyle='--', label='User Threshold (0.7)')\n",
    "    plt.axhline(y=0.75, color='orange', linestyle='--', label='Admin Threshold (0.75)')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Similarity')\n",
    "    plt.title('Threshold Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nRecommended thresholds:\")\n",
    "    print(f\"  - User registered: 0.70 (덜 엄격)\")\n",
    "    print(f\"  - Admin standard: 0.75 (더 엄격)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 성능 벤치마크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if os.path.exists(test_image_path):\n",
    "    # 단일 이미지 추론 속도\n",
    "    num_runs = 100\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = generate_embedding(test_image_path, model, processor, device)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs * 1000  # ms\n",
    "    \n",
    "    print(f\"Single image embedding generation:\")\n",
    "    print(f\"  Average time: {avg_time:.2f} ms\")\n",
    "    print(f\"  Throughput: {1000/avg_time:.1f} images/sec\")\n",
    "    \n",
    "    # 유사도 계산 속도\n",
    "    emb1 = generate_embedding(test_image_path, model, processor, device)\n",
    "    emb2 = generate_embedding(test_image_path, model, processor, device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(10000):\n",
    "        _ = calculate_cosine_similarity(emb1, emb2)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time_sim = (end_time - start_time) / 10000 * 1000000  # μs\n",
    "    \n",
    "    print(f\"\\nCosine similarity calculation:\")\n",
    "    print(f\"  Average time: {avg_time_sim:.2f} μs\")\n",
    "    print(f\"  Throughput: {1000000/avg_time_sim:.0f} comparisons/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 저장 및 로드 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# cup_code별 평균 임베딩을 JSON으로 저장\nimport json\n\nsave_dir = '../models/weights'\nos.makedirs(save_dir, exist_ok=True)\n\n# 평균 임베딩만 저장 (DB용)\nembeddings_db = {}\nfor cup_code, data in cup_code_embeddings.items():\n    embeddings_db[cup_code] = data['mean_embedding'].tolist()\n\n# JSON 저장\njson_path = os.path.join(save_dir, 'cup_code_embeddings.json')\nwith open(json_path, 'w') as f:\n    json.dump(embeddings_db, f, indent=2)\n\nprint(f\"✓ Saved {len(embeddings_db)} cup code embeddings to: {json_path}\")\n\n# 전체 데이터도 numpy로 저장 (분석용)\nfull_data_path = os.path.join(save_dir, 'cup_code_embeddings_full.npz')\nnp.savez_compressed(\n    full_data_path,\n    cup_codes=list(cup_code_embeddings.keys()),\n    mean_embeddings=np.array([data['mean_embedding'] for data in cup_code_embeddings.values()]),\n    num_images=np.array([data['num_images'] for data in cup_code_embeddings.values()])\n)\n\nprint(f\"✓ Saved full data to: {full_data_path}\")\n\n# 로드 테스트\nwith open(json_path, 'r') as f:\n    loaded_embeddings = json.load(f)\n\nprint(f\"\\nLoad test:\")\nprint(f\"  Loaded {len(loaded_embeddings)} cup codes\")\nprint(f\"  Sample cup_code: {list(loaded_embeddings.keys())[0]}\")\nprint(f\"  Embedding dimension: {len(loaded_embeddings[list(loaded_embeddings.keys())[0]])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. FastAPI 통합용 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_from_bytes(image_bytes, model, processor, device):\n",
    "    \"\"\"\n",
    "    바이트 데이터에서 직접 임베딩 생성 (FastAPI 통합용)\n",
    "    \n",
    "    Args:\n",
    "        image_bytes: 이미지 바이트 데이터\n",
    "        model: CLIP 모델\n",
    "        processor: CLIP 프로세서\n",
    "        device: 디바이스\n",
    "    \n",
    "    Returns:\n",
    "        list: 512차원 임베딩 벡터 (리스트 형태)\n",
    "    \"\"\"\n",
    "    from io import BytesIO\n",
    "    \n",
    "    # 바이트 → PIL Image\n",
    "    image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "    \n",
    "    # 전처리\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    # L2 정규화\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    \n",
    "    # 리스트로 변환 (JSON 직렬화 가능)\n",
    "    embedding = image_features.cpu().numpy().flatten().tolist()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# 테스트\n",
    "if os.path.exists(test_image_path):\n",
    "    with open(test_image_path, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    \n",
    "    embedding_list = generate_embedding_from_bytes(image_bytes, model, processor, device)\n",
    "    \n",
    "    print(f\"Embedding type: {type(embedding_list)}\")\n",
    "    print(f\"Embedding length: {len(embedding_list)}\")\n",
    "    print(f\"First 5 values: {embedding_list[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 요약\n\n### CLIP 임베딩 모델 특징\n- **차원**: 512\n- **정규화**: L2 norm = 1.0\n- **유사도**: 코사인 유사도 (내적 계산)\n- **추론 속도**: ~300ms/image (GPU)\n\n### 새로운 데이터셋 구조\n이제 `convert_labelstudio_to_dataset.py --include-types`로 생성된 데이터셋을 사용합니다:\n```\ndataset.zip\n└── types/\n    ├── CUP001/\n    ├── CUP002/\n    └── ...\n```\n\n### cup_code별 임베딩 생성\n1. 각 cup_code의 모든 이미지에서 임베딩 생성\n2. 평균 임베딩 계산 (대표 벡터)\n3. L2 정규화\n4. JSON으로 저장 → DB 또는 API에서 사용\n\n### 저장된 파일\n- `cup_code_embeddings.json`: cup_code → 평균 임베딩 (512차원)\n- `cup_code_embeddings_full.npz`: 전체 데이터 (분석용)\n\n### 권장 임계값\n- **사용자 등록 다회용기**: 0.70 (덜 엄격)\n- **관리자 표준 DB**: 0.75 (더 엄격)\n\n### 워크플로우\n1. Label Studio에서 어노테이션 → Export JSON\n2. `convert_labelstudio_to_dataset.py --include-types` 실행 → `types/` 생성\n3. 이 노트북 실행 → cup_code별 임베딩 생성\n4. `cup_code_embeddings.json`을 DB에 로드\n5. FastAPI에서 촬영 이미지 → 임베딩 → DB 매칭\n\n### 다음 단계\n1. FastAPI 서버에 통합 (`ai-server/models/embedding.py`)\n2. 실전 데이터로 임계값 조정\n3. 데이터베이스에 임베딩 저장 및 검색 최적화"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}