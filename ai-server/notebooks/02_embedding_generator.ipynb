{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP 기반 임베딩 생성 모델\n",
    "\n",
    "## 목표\n",
    "이미지를 512차원 벡터로 변환하여 유사도 비교에 사용\n",
    "\n",
    "## 모델\n",
    "- **백본**: CLIP (openai/clip-vit-base-patch32)\n",
    "- **출력**: 512차원 L2 정규화 벡터\n",
    "- **용도**: 다회용기 이미지 간 유사도 계산\n",
    "\n",
    "## 특징\n",
    "- 사전학습된 모델 사용 (학습 불필요)\n",
    "- 코사인 유사도로 이미지 비교\n",
    "- 빠른 추론 속도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CLIP 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP 모델 및 프로세서 로드\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "print(f\"Loading CLIP model: {model_name}\")\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"Embedding dimension: 512\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 임베딩 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(image_path, model, processor, device):\n",
    "    \"\"\"\n",
    "    이미지에서 512차원 임베딩 벡터 생성\n",
    "    \n",
    "    Args:\n",
    "        image_path: 이미지 파일 경로\n",
    "        model: CLIP 모델\n",
    "        processor: CLIP 프로세서\n",
    "        device: 디바이스 (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: L2 정규화된 512차원 벡터\n",
    "    \"\"\"\n",
    "    # 이미지 로딩\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # 전처리\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    # L2 정규화 (코사인 유사도 계산 최적화)\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    \n",
    "    # numpy 배열로 변환\n",
    "    embedding = image_features.cpu().numpy().flatten()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def generate_embedding_batch(image_paths, model, processor, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    여러 이미지의 임베딩을 배치로 생성\n",
    "    \n",
    "    Args:\n",
    "        image_paths: 이미지 파일 경로 리스트\n",
    "        model: CLIP 모델\n",
    "        processor: CLIP 프로세서\n",
    "        device: 디바이스\n",
    "        batch_size: 배치 크기\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: (N, 512) 형태의 임베딩 행렬\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc='Generating embeddings'):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        \n",
    "        # 이미지 로딩\n",
    "        images = [Image.open(path).convert('RGB') for path in batch_paths]\n",
    "        \n",
    "        # 전처리\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        \n",
    "        # L2 정규화\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        embeddings.append(image_features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 유사도 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    두 임베딩 벡터 간 코사인 유사도 계산\n",
    "    \n",
    "    Args:\n",
    "        embedding1: 첫 번째 임베딩 벡터\n",
    "        embedding2: 두 번째 임베딩 벡터\n",
    "    \n",
    "    Returns:\n",
    "        float: 코사인 유사도 (0.0 ~ 1.0)\n",
    "    \"\"\"\n",
    "    # L2 정규화되어 있으면 내적이 곧 코사인 유사도\n",
    "    similarity = np.dot(embedding1, embedding2)\n",
    "    return float(similarity)\n",
    "\n",
    "def find_most_similar(query_embedding, database_embeddings, threshold=0.7):\n",
    "    \"\"\"\n",
    "    데이터베이스에서 가장 유사한 임베딩 찾기\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: 쿼리 임베딩 (512,)\n",
    "        database_embeddings: DB 임베딩 행렬 (N, 512)\n",
    "        threshold: 최소 유사도 임계값\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (가장 유사한 인덱스, 유사도) 또는 (None, 0.0)\n",
    "    \"\"\"\n",
    "    if len(database_embeddings) == 0:\n",
    "        return None, 0.0\n",
    "    \n",
    "    # 배치 코사인 유사도 계산\n",
    "    similarities = np.dot(database_embeddings, query_embedding)\n",
    "    \n",
    "    # 최대값 찾기\n",
    "    max_idx = np.argmax(similarities)\n",
    "    max_similarity = similarities[max_idx]\n",
    "    \n",
    "    # 임계값 체크\n",
    "    if max_similarity >= threshold:\n",
    "        return int(max_idx), float(max_similarity)\n",
    "    else:\n",
    "        return None, float(max_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 테스트: 단일 이미지 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지 경로 (직접 지정)\n",
    "test_image_path = '../data/test_images/tumbler1.jpg'\n",
    "\n",
    "# 임베딩 생성\n",
    "embedding = generate_embedding(test_image_path, model, processor, device)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding norm (should be ~1.0): {np.linalg.norm(embedding):.4f}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "\n",
    "# 이미지 표시\n",
    "img = Image.open(test_image_path)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Test Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 유사도 테스트: 같은 물체 vs 다른 물체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지 3개 준비\n",
    "# - tumbler1.jpg: 기준 텀블러\n",
    "# - tumbler2.jpg: 같은 텀블러 (다른 각도)\n",
    "# - cup.jpg: 다른 컵\n",
    "\n",
    "image1_path = '../data/test_images/tumbler1.jpg'\n",
    "image2_path = '../data/test_images/tumbler2.jpg'  # 같은 물체\n",
    "image3_path = '../data/test_images/cup.jpg'       # 다른 물체\n",
    "\n",
    "# 임베딩 생성\n",
    "emb1 = generate_embedding(image1_path, model, processor, device)\n",
    "emb2 = generate_embedding(image2_path, model, processor, device)\n",
    "emb3 = generate_embedding(image3_path, model, processor, device)\n",
    "\n",
    "# 유사도 계산\n",
    "sim_same = calculate_cosine_similarity(emb1, emb2)\n",
    "sim_diff = calculate_cosine_similarity(emb1, emb3)\n",
    "\n",
    "print(f\"Similarity (same object, different angle): {sim_same:.4f}\")\n",
    "print(f\"Similarity (different object): {sim_diff:.4f}\")\n",
    "print(f\"\\nDifference: {sim_same - sim_diff:.4f}\")\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(Image.open(image1_path))\n",
    "axes[0].set_title('Reference Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(Image.open(image2_path))\n",
    "axes[1].set_title(f'Same Object\\nSimilarity: {sim_same:.3f}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(Image.open(image3_path))\n",
    "axes[2].set_title(f'Different Object\\nSimilarity: {sim_diff:.3f}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 배치 임베딩 생성 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 이미지 경로 준비\n",
    "test_dir = '../data/test_images'\n",
    "image_paths = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "print(f\"Found {len(image_paths)} images\")\n",
    "\n",
    "# 배치 임베딩 생성\n",
    "embeddings = generate_embedding_batch(image_paths, model, processor, device, batch_size=8)\n",
    "\n",
    "print(f\"\\nGenerated embeddings shape: {embeddings.shape}\")\n",
    "print(f\"All norms ≈ 1.0: {np.allclose(np.linalg.norm(embeddings, axis=1), 1.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 유사도 행렬 계산 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# 유사도 행렬 계산\n",
    "similarity_matrix = np.dot(embeddings, embeddings.T)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            xticklabels=[os.path.basename(p) for p in image_paths],\n",
    "            yticklabels=[os.path.basename(p) for p in image_paths])\n",
    "plt.title('Cosine Similarity Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. t-SNE 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE로 2D 투영\n",
    "if len(embeddings) >= 3:\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100)\n",
    "    \n",
    "    for i, path in enumerate(image_paths):\n",
    "        plt.annotate(os.path.basename(path), \n",
    "                    (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    fontsize=8)\n",
    "    \n",
    "    plt.title('t-SNE Visualization of Image Embeddings')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 3 images for t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 실전 시나리오: 다회용기 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시나리오:\n",
    "# 1. 사용자가 다회용기 3개를 등록 (데이터베이스)\n",
    "# 2. 사용 인증 시 촬영한 이미지가 어느 다회용기인지 매칭\n",
    "\n",
    "# 데이터베이스: 사용자 등록 다회용기 3개\n",
    "registered_paths = [\n",
    "    '../data/registered/tumbler_a.jpg',\n",
    "    '../data/registered/tumbler_b.jpg',\n",
    "    '../data/registered/tumbler_c.jpg',\n",
    "]\n",
    "\n",
    "# 등록 다회용기 임베딩 생성\n",
    "registered_embeddings = []\n",
    "for path in registered_paths:\n",
    "    if os.path.exists(path):\n",
    "        emb = generate_embedding(path, model, processor, device)\n",
    "        registered_embeddings.append(emb)\n",
    "\n",
    "if len(registered_embeddings) > 0:\n",
    "    registered_embeddings = np.array(registered_embeddings)\n",
    "    \n",
    "    # 쿼리: 사용 인증 시 촬영한 이미지\n",
    "    query_path = '../data/usage/query_tumbler.jpg'\n",
    "    \n",
    "    if os.path.exists(query_path):\n",
    "        query_embedding = generate_embedding(query_path, model, processor, device)\n",
    "        \n",
    "        # 매칭\n",
    "        matched_idx, similarity = find_most_similar(query_embedding, registered_embeddings, threshold=0.7)\n",
    "        \n",
    "        if matched_idx is not None:\n",
    "            print(f\"✓ Matched with registered tumbler #{matched_idx}\")\n",
    "            print(f\"  Similarity: {similarity:.4f}\")\n",
    "            print(f\"  Path: {registered_paths[matched_idx]}\")\n",
    "        else:\n",
    "            print(f\"✗ No match found\")\n",
    "            print(f\"  Highest similarity: {similarity:.4f} (below threshold 0.7)\")\n",
    "    else:\n",
    "        print(f\"Query image not found: {query_path}\")\n",
    "else:\n",
    "    print(\"No registered images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 임계값 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 임계값에서 매칭 결과 분석\n",
    "thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "\n",
    "if len(registered_embeddings) > 0 and os.path.exists(query_path):\n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        matched_idx, similarity = find_most_similar(query_embedding, registered_embeddings, threshold=threshold)\n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'matched': matched_idx is not None,\n",
    "            'similarity': similarity\n",
    "        })\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, [r['similarity'] for r in results], 'b-', linewidth=2, label='Similarity Score')\n",
    "    plt.axhline(y=0.7, color='r', linestyle='--', label='User Threshold (0.7)')\n",
    "    plt.axhline(y=0.75, color='orange', linestyle='--', label='Admin Threshold (0.75)')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Similarity')\n",
    "    plt.title('Threshold Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nRecommended thresholds:\")\n",
    "    print(f\"  - User registered: 0.70 (덜 엄격)\")\n",
    "    print(f\"  - Admin standard: 0.75 (더 엄격)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 성능 벤치마크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if os.path.exists(test_image_path):\n",
    "    # 단일 이미지 추론 속도\n",
    "    num_runs = 100\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = generate_embedding(test_image_path, model, processor, device)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs * 1000  # ms\n",
    "    \n",
    "    print(f\"Single image embedding generation:\")\n",
    "    print(f\"  Average time: {avg_time:.2f} ms\")\n",
    "    print(f\"  Throughput: {1000/avg_time:.1f} images/sec\")\n",
    "    \n",
    "    # 유사도 계산 속도\n",
    "    emb1 = generate_embedding(test_image_path, model, processor, device)\n",
    "    emb2 = generate_embedding(test_image_path, model, processor, device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(10000):\n",
    "        _ = calculate_cosine_similarity(emb1, emb2)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time_sim = (end_time - start_time) / 10000 * 1000000  # μs\n",
    "    \n",
    "    print(f\"\\nCosine similarity calculation:\")\n",
    "    print(f\"  Average time: {avg_time_sim:.2f} μs\")\n",
    "    print(f\"  Throughput: {1000000/avg_time_sim:.0f} comparisons/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 저장 및 로드 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩을 파일로 저장 (numpy)\n",
    "save_path = '../models/weights/sample_embeddings.npy'\n",
    "\n",
    "if len(embeddings) > 0:\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    np.save(save_path, embeddings)\n",
    "    print(f\"Embeddings saved to: {save_path}\")\n",
    "    \n",
    "    # 로드 테스트\n",
    "    loaded_embeddings = np.load(save_path)\n",
    "    print(f\"Loaded embeddings shape: {loaded_embeddings.shape}\")\n",
    "    print(f\"Data match: {np.allclose(embeddings, loaded_embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. FastAPI 통합용 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_from_bytes(image_bytes, model, processor, device):\n",
    "    \"\"\"\n",
    "    바이트 데이터에서 직접 임베딩 생성 (FastAPI 통합용)\n",
    "    \n",
    "    Args:\n",
    "        image_bytes: 이미지 바이트 데이터\n",
    "        model: CLIP 모델\n",
    "        processor: CLIP 프로세서\n",
    "        device: 디바이스\n",
    "    \n",
    "    Returns:\n",
    "        list: 512차원 임베딩 벡터 (리스트 형태)\n",
    "    \"\"\"\n",
    "    from io import BytesIO\n",
    "    \n",
    "    # 바이트 → PIL Image\n",
    "    image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "    \n",
    "    # 전처리\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    # L2 정규화\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    \n",
    "    # 리스트로 변환 (JSON 직렬화 가능)\n",
    "    embedding = image_features.cpu().numpy().flatten().tolist()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# 테스트\n",
    "if os.path.exists(test_image_path):\n",
    "    with open(test_image_path, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    \n",
    "    embedding_list = generate_embedding_from_bytes(image_bytes, model, processor, device)\n",
    "    \n",
    "    print(f\"Embedding type: {type(embedding_list)}\")\n",
    "    print(f\"Embedding length: {len(embedding_list)}\")\n",
    "    print(f\"First 5 values: {embedding_list[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "### CLIP 임베딩 모델 특징\n",
    "- **차원**: 512\n",
    "- **정규화**: L2 norm = 1.0\n",
    "- **유사도**: 코사인 유사도 (내적 계산)\n",
    "- **추론 속도**: ~300ms/image (GPU)\n",
    "\n",
    "### 권장 임계값\n",
    "- **사용자 등록 다회용기**: 0.70 (덜 엄격)\n",
    "- **관리자 표준 DB**: 0.75 (더 엄격)\n",
    "\n",
    "### 다음 단계\n",
    "1. FastAPI 서버에 통합 (`ai-server/models/embedding.py`)\n",
    "2. 실전 데이터로 임계값 조정\n",
    "3. 데이터베이스에 임베딩 저장 및 검색 최적화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
